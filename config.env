# =============================================================================
# LLM Watchdog Configuration for Gilbreth
# =============================================================================
# Model-specific settings live in models/*.env
# Switch models with:  ./switch-model.sh <model-name>
# Active model:        active-model.env -> models/<name>.env

# -----------------------------------------------------------------------------
# Notifications (ntfy.sh)
# -----------------------------------------------------------------------------
NTFY_TOPIC="purduechat-watchdog"
NTFY_SERVER="https://ntfy.sh"

# -----------------------------------------------------------------------------
# CUDA Environment (per-node, uncomment if node needs CUDA overrides)
# -----------------------------------------------------------------------------
# Nodes with CUDA < 12.8 need overrides for FP8 models. Set CUDA_SETUP="true"
# and configure paths below. Nodes with CUDA >= 12.8 can leave this off.
#CUDA_SETUP="true"
#CUDA12_LIB="/apps/spack/gilbreth-r9/apps/cuda/12.6.0-gcc-11.5.0-a7cv7sp/lib64"
#CUDA_HOME_OVERRIDE="/apps/external/cuda-toolkit/13.1.0"
#LLM_VENV="/home/dchawra/llm-watchdog/cu13/.venv"  # override venv for CUDA 13

# -----------------------------------------------------------------------------
# LLM Server (vLLM)
# -----------------------------------------------------------------------------
LLM_VENV="/home/dchawra/llm-watchdog/.venv"
LLM_BIN="${LLM_VENV}/bin/vllm"
SERVER_PORT="8000"
HF_HOME="/scratch/gilbreth/dchawra/cache/huggingface"

# -----------------------------------------------------------------------------
# Cloudflare Tunnel
# -----------------------------------------------------------------------------
CLOUDFLARED_BIN="/home/dchawra/cloudflared-linux-amd64"
TUNNEL_NAME="gilbreth"
TUNNEL_ID="0051dc2e-f4fb-4a69-8284-6c18d35514fe"
TUNNEL_HOSTNAME="purduechat.dwijen.dev"
CLOUDFLARED_CREDS="/home/dchawra/.cloudflared/${TUNNEL_ID}.json"

# -----------------------------------------------------------------------------
# GPU Allocation
# -----------------------------------------------------------------------------
# How many GPUs to claim for vLLM. On nodes with more GPUs than this,
# the watchdog coexists with Slurm jobs on the remaining GPUs.
NUM_GPUS=4

# -----------------------------------------------------------------------------
# Detection Settings
# -----------------------------------------------------------------------------
# Polling interval for cgroup monitoring (when inotify unavailable)
POLL_INTERVAL="0.25"

# Available nodes for migration (space-separated, reverse order priority)
AVAILABLE_NODES=(i000 i001 i002)
