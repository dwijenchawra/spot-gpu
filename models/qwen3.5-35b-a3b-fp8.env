# =============================================================================
# Qwen3.5-35B-A3B-FP8  --  Qwen/Qwen3.5-35B-A3B-FP8
# =============================================================================
# FP8 MoE model (35B total, 3B active). Requires CUDA >= 12.8.
# Thinking mode tuned for precise coding tasks (WebDev).
# Fits on a single GPU.

MODEL_NAME="Qwen/Qwen3.5-35B-A3B-FP8"

VLLM_ARGS="--tensor-parallel-size 1 \
    --max-model-len 262144 \
    --speculative-config.method qwen3_next_mtp \
    --speculative-config.num_speculative_tokens 2 \
    --tool-call-parser qwen3_coder \
    --reasoning-parser qwen3 \
    --enable-auto-tool-choice \
    --override-generation-config.temperature 0.6 \
    --override-generation-config.top_p 0.95 \
    --override-generation-config.top_k 20 \
    --override-generation-config.min_p 0.0 \
    --override-generation-config.presence_penalty 0.0 \
    --override-generation-config.repetition_penalty 1.0 \
    --served-model-name qwen3.5-35b \
    --load-format fastsafetensors"
